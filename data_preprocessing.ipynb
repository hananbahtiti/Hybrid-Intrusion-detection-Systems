{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-zW0--TET_w0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hananbahtiti/Hybrid-Intrusion-detection-Systems/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Dataset"
      ],
      "metadata": {
        "id": "-zW0--TET_w0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDSozQi9-uwW",
        "outputId": "b042dd43-2b0f-4a36-871a-8ca347f4cb52",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-22 16:40:01--  https://unsworks.unsw.edu.au/bitstreams/0ac2820a-5131-43ab-90b2-c624c8d73649/download\n",
            "Resolving unsworks.unsw.edu.au (unsworks.unsw.edu.au)... 54.253.215.118\n",
            "Connecting to unsworks.unsw.edu.au (unsworks.unsw.edu.au)|54.253.215.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://unsworks.unsw.edu.au/server/api/core/bitstreams/0ac2820a-5131-43ab-90b2-c624c8d73649/content [following]\n",
            "--2025-04-22 16:40:02--  https://unsworks.unsw.edu.au/server/api/core/bitstreams/0ac2820a-5131-43ab-90b2-c624c8d73649/content\n",
            "Reusing existing connection to unsworks.unsw.edu.au:443.\n",
            "HTTP request sent, awaiting response... 200 200\n",
            "Length: unspecified [application/x-rar-compressed]\n",
            "Saving to: ‘NGIDS-DS.rar’\n",
            "\n",
            "NGIDS-DS.rar            [               <=>  ] 941.51M  14.5MB/s    in 63s     \n",
            "\n",
            "2025-04-22 16:41:05 (15.0 MB/s) - ‘NGIDS-DS.rar’ saved [987249484]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O NGIDS-DS.rar https://unsworks.unsw.edu.au/bitstreams/0ac2820a-5131-43ab-90b2-c624c8d73649/download"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unrar x /content/NGIDS-DS.rar"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XyP_2zMAbEn",
        "outputId": "61a064c3-b6df-416c-9677-296f3e2dd086"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/NGIDS-DS.rar\n",
            "\n",
            "Creating    NGIDS                                                     OK\n",
            "Creating    NGIDS/NGIDS-DS-v1                                         OK\n",
            "Extracting  NGIDS/NGIDS-DS-v1/feature_descr.csv                          \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/ground_truth.csv                           \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Creating    NGIDS/NGIDS-DS-v1/host logs                               OK\n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/1.csv                            \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/10.csv                           \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/11.csv                           \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/12.csv                           \b\b\b\b  1%\b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/13.csv                           \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/14.csv                           \b\b\b\b  2%\b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/15.csv                           \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/16.csv                           \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/17.csv                           \b\b\b\b  3%\b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/18.csv                           \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/19.csv                           \b\b\b\b  4%\b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/2.csv                            \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/20.csv                           \b\b\b\b  5%\b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/21.csv                           \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/22.csv                           \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/23.csv                           \b\b\b\b  6%\b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/24.csv                           \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/25.csv                           \b\b\b\b  7%\b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/26.csv                           \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/27.csv                           \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/28.csv                           \b\b\b\b  8%\b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/29.csv                           \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/3.csv                            \b\b\b\b  9%\b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/30.csv                           \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/31.csv                           \b\b\b\b 10%\b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/32.csv                           \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/33.csv                           \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/34.csv                           \b\b\b\b 11%\b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/35.csv                           \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/36.csv                           \b\b\b\b 12%\b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/37.csv                           \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/38.csv                           \b\b\b\b 13%\b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/39.csv                           \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/4.csv                            \b\b\b\b 14%\b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/40.csv                           \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/41.csv                           \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/42.csv                           \b\b\b\b 15%\b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/43.csv                           \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/44.csv                           \b\b\b\b 16%\b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/45.csv                           \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/46.csv                           \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/47.csv                           \b\b\b\b 17%\b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/48.csv                           \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/49.csv                           \b\b\b\b 18%\b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/5.csv                            \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/50.csv                           \b\b\b\b 19%\b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/51.csv                           \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/52.csv                           \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/53.csv                           \b\b\b\b 20%\b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/54.csv                           \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/55.csv                           \b\b\b\b 21%\b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/56.csv                           \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/57.csv                           \b\b\b\b 22%\b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/58.csv                           \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/59.csv                           \b\b\b\b 23%\b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/6.csv                            \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/60.csv                           \b\b\b\b 24%\b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/61.csv                           \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/62.csv                           \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/63.csv                           \b\b\b\b 25%\b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/64.csv                           \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/65.csv                           \b\b\b\b 26%\b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/66.csv                           \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/67.csv                           \b\b\b\b 27%\b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/68.csv                           \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/69.csv                           \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/7.csv                            \b\b\b\b 28%\b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/70.csv                           \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/71.csv                           \b\b\b\b 29%\b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/72.csv                           \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/73.csv                           \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/74.csv                           \b\b\b\b 30%\b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/75.csv                           \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/76.csv                           \b\b\b\b 31%\b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/77.csv                           \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/78.csv                           \b\b\b\b 32%\b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/79.csv                           \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/8.csv                            \b\b\b\b 33%\b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/80.csv                           \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/81.csv                           \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/82.csv                           \b\b\b\b 34%\b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/83.csv                           \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/84.csv                           \b\b\b\b 35%\b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/85.csv                           \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/86.csv                           \b\b\b\b 36%\b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/87.csv                           \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/88.csv                           \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/89.csv                           \b\b\b\b 37%\b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/9.csv                            \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/90.csv                           \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/91.csv                           \b\b\b\b 38%\b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/92.csv                           \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/93.csv                           \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/94.csv                           \b\b\b\b 39%\b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/95.csv                           \b\b\b\b 40%\b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/96.csv                           \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/97.csv                           \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/98.csv                           \b\b\b\b 41%\b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/host logs/99.csv                           \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/NGIDS.pcap                                 \b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v1/Readme.txt                                 \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    NGIDS/NGIDS-DS-v2                                         OK\n",
            "Extracting  NGIDS/NGIDS-DS-v2/readme.txt                                 \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Creating    NGIDS/NGIDS-DS-v2/Test data                               OK\n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_1.csv                          \b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_10.csv                         \b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_11.csv                         \b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_12.csv                         \b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_13.csv                         \b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_14.csv                         \b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_15.csv                         \b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_16.csv                         \b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_2.csv                          \b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_3.csv                          \b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_4.csv                          \b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_5.csv                          \b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_6.csv                          \b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_7.csv                          \b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_8.csv                          \b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Test data/0_9.csv                          \b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Creating    NGIDS/NGIDS-DS-v2/Traing data                             OK\n",
            "Extracting  NGIDS/NGIDS-DS-v2/Traing data/0_1.csv                        \b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Traing data/0_2.csv                        \b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Traing data/0_3.csv                        \b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/NGIDS-DS-v2/Traing data/0_4.csv                        \b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  NGIDS/Readme.txt                                             \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Host data processing"
      ],
      "metadata": {
        "id": "zJg_zDVfT9Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/NGIDS/NGIDS-DS-v1/host logs/1.csv')\n",
        "df.info()\n",
        "df.iloc[:,3].head()"
      ],
      "metadata": {
        "id": "EOOQJvLHX9rl",
        "outputId": "042d3c6f-3d57-4178-d696-caebc2bf3b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 9 columns):\n",
            " #   Column                     Non-Null Count    Dtype \n",
            "---  ------                     --------------    ----- \n",
            " 0   11/03/2016                 1000000 non-null  object\n",
            " 1   2:45:01                    1000000 non-null  object\n",
            " 2   1830                       1000000 non-null  int64 \n",
            " 3   /sbin/upstart-dbus-bridge  1000000 non-null  object\n",
            " 4   142                        1000000 non-null  int64 \n",
            " 5   45354                      1000000 non-null  int64 \n",
            " 6   normal                     1000000 non-null  object\n",
            " 7   normal.1                   1000000 non-null  object\n",
            " 8   0                          1000000 non-null  int64 \n",
            "dtypes: int64(4), object(5)\n",
            "memory usage: 68.7+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                          /bin/dbus-daemon\n",
              "1    /usr/lib/i386-linux-gnu/gconf/gconfd-2\n",
              "2                        /usr/bin/python3.4\n",
              "3                      /usr/bin/ibus-daemon\n",
              "4                /usr/lib/ibus/ibus-ui-gtk3\n",
              "Name: /sbin/upstart-dbus-bridge, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>/sbin/upstart-dbus-bridge</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/bin/dbus-daemon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/usr/lib/i386-linux-gnu/gconf/gconfd-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/usr/bin/python3.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/usr/bin/ibus-daemon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/usr/lib/ibus/ibus-ui-gtk3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv('/content/NGIDS/NGIDS-DS-v1/host logs/1.csv')\n",
        "\n",
        "df_split = df.iloc[:,3].str.split('/', expand=True)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "for col in df_split.columns:\n",
        "    df_split[col] = encoder.fit_transform(df_split[col].fillna(''))\n",
        "\n",
        "df = pd.concat([df, df_split], axis=1)\n",
        "\n",
        "\n",
        "df.info()\n",
        "#df.head()\n",
        "#df.iloc[:,9]"
      ],
      "metadata": {
        "id": "tr9XKW-icps0",
        "outputId": "e2e09f9e-55ef-459a-a13a-3b08e3bc3b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 15 columns):\n",
            " #   Column                     Non-Null Count    Dtype \n",
            "---  ------                     --------------    ----- \n",
            " 0   11/03/2016                 1000000 non-null  object\n",
            " 1   2:45:01                    1000000 non-null  object\n",
            " 2   1830                       1000000 non-null  int64 \n",
            " 3   /sbin/upstart-dbus-bridge  1000000 non-null  object\n",
            " 4   142                        1000000 non-null  int64 \n",
            " 5   45354                      1000000 non-null  int64 \n",
            " 6   normal                     1000000 non-null  object\n",
            " 7   normal.1                   1000000 non-null  object\n",
            " 8   0                          1000000 non-null  int64 \n",
            " 9   0                          1000000 non-null  int64 \n",
            " 10  1                          1000000 non-null  int64 \n",
            " 11  2                          1000000 non-null  int64 \n",
            " 12  3                          1000000 non-null  int64 \n",
            " 13  4                          1000000 non-null  int64 \n",
            " 14  5                          1000000 non-null  int64 \n",
            "dtypes: int64(10), object(5)\n",
            "memory usage: 114.4+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "999995    0\n",
              "999996    0\n",
              "999997    0\n",
              "999998    0\n",
              "999999    0\n",
              "Name: 0, Length: 1000000, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "RyP2JfTfXV2w",
        "outputId": "5857b7b6-ae2f-44ff-bca5-119807e51539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "UJ0W3F-qT6Td"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HostPreprocessing():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __file_path_collection(self, folder_path):\n",
        "    try:\n",
        "      joined_list = []\n",
        "      for file_name in os.listdir(folder_path):\n",
        "        file_name = os.path.join(folder_path, file_name)\n",
        "        if os.path.isfile(file_name) and file_name.endswith('.csv'):\n",
        "          joined_list.append(file_name)\n",
        "      return joined_list\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "  def rename_columns(self, folder_path):\n",
        "    files = self.__file_path_collection(folder_path)\n",
        "    try:\n",
        "      for file in tqdm(files, desc=\"Renaming columns\") :\n",
        "        df = pd.read_csv(file)\n",
        "        column_name = df.columns.tolist()\n",
        "        df.rename(columns={\n",
        "            column_name[0]:'Date',\n",
        "            column_name[1]:'Time',\n",
        "            column_name[2]:'Unique_Identification',\n",
        "            column_name[3]:'Execution_Path',\n",
        "            column_name[4]:'System_Calls_Identifiers',\n",
        "            column_name[5]:\"Event's_Unique_Identification\",\n",
        "            column_name[6]:'attacks',\n",
        "            column_name[7]:\"Sub_Type_Attack\",\n",
        "            column_name[8]:\"Label\"\n",
        "            }, inplace=True)\n",
        "        df.to_csv(file, index=False)\n",
        "      return f\"done files...\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "  def data_encoding(self, folder_path, column_number : int):\n",
        "    files = self.__file_path_collection(folder_path)\n",
        "    encoder = LabelEncoder()\n",
        "    all_value = []\n",
        "    try:\n",
        "      for file in tqdm(files, desc=\"Extract all values from files\"):\n",
        "        df = pd.read_csv(file)\n",
        "        all_value.extend(df.iloc[:, column_number].dropna().unique())\n",
        "        #print( len(df.iloc[:, column_number].dropna().unique()) , file )\n",
        "      encoder.fit(all_value)\n",
        "\n",
        "      for file in tqdm(files, desc=\"encoder data\"):\n",
        "        df = pd.read_csv(file)\n",
        "        df.iloc[:, column_number] = encoder.transform(df.iloc[:, column_number])\n",
        "        #print(len(df.iloc[:, column_number].dropna().unique()), df.iloc[:, column_number].dropna().unique(), file )\n",
        "        df.to_csv(file, index=False)\n",
        "      return f\"done file...\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "  def paths_encoding(self, folder_path, column_number: int):\n",
        "    files = self.__file_path_collection(folder_path)\n",
        "    all_unique_value = []\n",
        "    unique_value = []\n",
        "    try:\n",
        "      for file in tqdm(files, desc=\"read all files\"):\n",
        "        df = pd.read_csv(file)\n",
        "        unique_value.extend(df.iloc[:,column_number].unique())\n",
        "        #df[\"Tokenized\"] = unique_value.apply(lambda p: p.strip(\"/\").split(\"/\"))\n",
        "        for tokens in unique_value:\n",
        "          all_unique_value.extend(tokens)\n",
        "\n",
        "      print( len(all_unique_value), all_unique_value)\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "  def file_collection(self, folder_path):\n",
        "    try:\n",
        "      file_paths = self.__file_path_collection(folder_path)\n",
        "      if file_paths:\n",
        "        output_folder = os.path.join(os.getcwd(),'files')\n",
        "        os.makedirs(output_folder ,exist_ok=True)\n",
        "        output_file = os.path.join(output_folder,\"final.csv\")\n",
        "\n",
        "        for file in tqdm(file_paths, desc=\"read all files\"):\n",
        "\n",
        "          first = True\n",
        "          chunk = pd.read_csv(file, chunksize=10000)\n",
        "          for part in chunk:\n",
        "            part.to_csv(output_file, mode='a', header=first, index=False)\n",
        "            first = False\n",
        "\n",
        "      return  f\"done file...\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "a = HostPreprocessing()\n",
        "#a.file_path_collection(\"/content/NGIDS/NGIDS-DS-v1/host logs\")\n",
        "a.file_collection(\"/content/NGIDS/NGIDS-DS-v1/host logs\")\n",
        "#a.rename_columns( '/content/NGIDS/NGIDS-DS-v1/host logs')\n",
        "#a.data_encoding('/content/NGIDS/NGIDS-DS-v1/host logs', column_number=6)\n",
        "#a.data_encoding('/content/NGIDS/NGIDS-DS-v1/host logs', column_number=7)\n",
        "\n",
        "#a.paths_encoding('/content/NGIDS/NGIDS-DS-v1/host logs', column_number=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iadsIYxGUDEL",
        "outputId": "5745db6d-f8dc-4745-cd3b-e0398729e43c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "read all files: 100%|██████████| 99/99 [09:22<00:00,  5.68s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'done file...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yP5t2mKKWsNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scapy"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4kD82xplN91",
        "outputId": "1b79c674-9fb5-4563-d4e8-4c0f88f014c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scapy\n",
            "  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Downloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scapy\n",
            "Successfully installed scapy-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from scapy.all import PcapReader"
      ],
      "metadata": {
        "id": "7EZjcRDSWvNI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkPreprocessing:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def extract_tcp_option(self, option, key):\n",
        "    for opt in option:\n",
        "      if opt[0] == key:\n",
        "        return opt[1]\n",
        "    return None\n",
        "\n",
        "\n",
        "  def delete_data(self, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.loc[:, df.nunique() > 1]\n",
        "    df.to_csv(file_path, index=False)\n",
        "    return f\"Columns containing only one value are deleted...\"\n",
        "\n",
        "  def convert_pcap_csv(self, pcap_file, csv_file):\n",
        "    try:\n",
        "      print(\"Countig packages...\")\n",
        "      with PcapReader(pcap_file) as count_reader:\n",
        "        total_packets = sum(1 for _ in count_reader)\n",
        "        print(f\"Total number of packages: {total_packets}\")\n",
        "\n",
        "      print(\"Start converting to csv file...\")\n",
        "      with PcapReader(pcap_file) as packets:\n",
        "        with open(csv_file, mode='w', newline=\"\") as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\n",
        "              \"Packet Number\", \"Ethernet DST\", \"Ethernet SRC\", \"Ethernet Type\",\n",
        "              \"IP Version\", \"IP IHL\", \"IP TOS\", \"IP Length\", \"IP ID\", \"IP Flags\",\n",
        "              \"IP Fragmentation\", \"IP TTL\", \"IP Proto\", \"IP Checksum\",\n",
        "              \"IP Src\", \"IP Dst\", \"TCP Src Port\", \"TCP Dst Port\", \"TCP Seq\",\n",
        "              \"TCP Ack\", \"TCP Data Offset\", \"TCP Reserved\", \"TCP Flags\",\n",
        "              \"TCP Window\", \"TCP Checksum\", \"TCP Urgent Pointer\", \"TCP Options MSS\",\n",
        "              \"TCP Options WScale\", \"TCP Options NOP\",\"Hexdump\"\n",
        "          ])\n",
        "\n",
        "          for i, packet in enumerate(tqdm(packets,total=total_packets, desc=\"Packet processing\")):\n",
        "            if packet.haslayer('Ethernet'):\n",
        "              eth_dst = packet['Ethernet'].dst\n",
        "              eth_src = packet['Ethernet'].src\n",
        "              eth_type = packet['Ethernet'].type\n",
        "            else:\n",
        "              eth_dst = eth_src = eth_type = None\n",
        "\n",
        "            if packet.haslayer('IP'):\n",
        "              ip_version = packet['IP'].version\n",
        "              ip_ihl = packet['IP'].ihl\n",
        "              ip_tos = packet['IP'].tos\n",
        "              ip_len = packet['IP'].len\n",
        "              ip_id = packet['IP'].id\n",
        "              ip_flags = packet['IP'].flags\n",
        "              ip_frag = packet['IP'].frag\n",
        "              ip_ttl = packet['IP'].ttl\n",
        "              ip_proto = packet['IP'].proto\n",
        "              ip_chksum = packet['IP'].chksum\n",
        "              ip_src = packet['IP'].src\n",
        "              ip_dst = packet['IP'].dst\n",
        "            else:\n",
        "              ip_version = ip_ihl = ip_tos = ip_len = ip_id = ip_flags = ip_frag = ip_ttl = ip_proto = ip_chksum = ip_src = ip_dst = None\n",
        "\n",
        "            if packet.haslayer('TCP'):\n",
        "              tcp_sport = packet['TCP'].sport\n",
        "              tcp_dport = packet['TCP'].dport\n",
        "              tcp_seq = packet['TCP'].seq\n",
        "              tcp_ack = packet['TCP'].ack\n",
        "              tcp_dataofs = packet['TCP'].dataofs\n",
        "              tcp_reserved = packet['TCP'].reserved\n",
        "              tcp_flags = packet['TCP'].flags\n",
        "              tcp_window = packet['TCP'].window\n",
        "              tcp_chksum = packet['TCP'].chksum\n",
        "              tcp_urgptr = packet['TCP'].urgptr\n",
        "              tcp_options = packet['TCP'].options\n",
        "\n",
        "              tcp_mss = self.extract_tcp_option(tcp_options, 'MSS')\n",
        "              tcp_wscale = self.extract_tcp_option(tcp_options, 'WScale')\n",
        "              tcp_nop_count = self.extract_tcp_option(tcp_options, 'NOP')\n",
        "\n",
        "            else:\n",
        "              tcp_sport = tcp_dport = tcp_seq = tcp_ack = tcp_dataofs = tcp_reserved = tcp_flags = tcp_window = tcp_chksum = tcp_urgptr = tcp_mss = tcp_wscale = tcp_nop_count = None\n",
        "\n",
        "            raw_data = packet.original.hex()\n",
        "            writer.writerow([\n",
        "                i + 1, eth_dst, eth_src, eth_type,\n",
        "                  ip_version, ip_ihl, ip_tos, ip_len, ip_id, ip_flags,\n",
        "                  ip_frag, ip_ttl, ip_proto, ip_chksum,\n",
        "                  ip_src, ip_dst, tcp_sport, tcp_dport, tcp_seq,\n",
        "                  tcp_ack, tcp_dataofs, tcp_reserved, tcp_flags,\n",
        "                  tcp_window, tcp_chksum, tcp_urgptr, tcp_mss, tcp_wscale, tcp_nop_count,\n",
        "                  raw_data\n",
        "            ])\n",
        "\n",
        "      return f\"The file has been converted to csv successfully.\"\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "a = NetworkPreprocessing()\n",
        "#a.convert_pcap_csv(pcap_file='/content/NGIDS/NGIDS-DS-v1/NGIDS.pcap',\n",
        "#                   csv_file='/content/drive/MyDrive/hybrid_IDS/network.csv')\n",
        "#a.delete_data(file_path='/content/drive/MyDrive/hybrid_IDS/network.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bJW0krgnXThk",
        "outputId": "c6ab4c5e-07c8-46cd-a10f-263ed88555ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Columns containing only one value are deleted...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PcapFileProcessing:\n",
        "  def __init__(self) -> None:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "  def delete_data(self, file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.loc[:, df.nunique() > 1]\n",
        "    df.to_csv(file_path, index=False)\n",
        "    return f\"Columns containing only one value are deleted...\"\n",
        "\n",
        "\n",
        "  def date_time_columns(self, file_path, csv_file):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, dtype=str)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        if 'timestamp' not in df.columns:\n",
        "            return \"Error: The 'timestamp' column is missing from the file.\"\n",
        "\n",
        "        df['timestamp'] = df['timestamp'].astype(str).str.strip()\n",
        "\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce', dayfirst=True)\n",
        "\n",
        "        if df['timestamp'].isnull().all():\n",
        "            return \"All values in 'timestamp' are invalid and could not be converted.\"\n",
        "\n",
        "        # Extract date and time\n",
        "        df['date'] = df['timestamp'].dt.strftime('%d/%m/%Y')\n",
        "        df['time'] = df['timestamp'].dt.strftime('%H:%M:%S')\n",
        "\n",
        "        # Save the result\n",
        "        df.to_csv(csv_file, index=False)\n",
        "\n",
        "        return \"Successfully converted 'timestamp' to 'date' and 'time'.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during conversion: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def convert_log_csv(self, log_file, csv_file):\n",
        "    try:\n",
        "      print(\"Start converting to csv file...\")\n",
        "      with open(log_file, \"r\") as logfile:\n",
        "        log_data = logfile.read()\n",
        "      pattern = re.compile(\n",
        "          r'(?P<timestamp>\\d{2}/\\d{2}/\\d{4}-\\d{2}:\\d{2}:\\d{2}\\.\\d+)\\s+\\[\\*\\*\\]\\s+\\[(?P<sid>[^]]+)\\]\\s+(?P<signature>.*?)\\s+\\[\\*\\*\\]\\s+\\[Classification:\\s+(?P<classification>.*?)\\]\\s+\\[Priority:\\s+(?P<priority>\\d+)\\]\\s+\\{(?P<protocol>\\w+)\\}\\s+(?P<src_ip>\\d+\\.\\d+\\.\\d+\\.\\d+):(?P<src_port>\\d+)\\s+->\\s+(?P<dst_ip>\\d+\\.\\d+\\.\\d+\\.\\d+):(?P<dst_port>\\d+)'\n",
        "          )\n",
        "      with open(csv_file, \"w\", newline=\"\") as csvfile:\n",
        "        fieldnames = ['timestamp', 'sid', 'signature', 'classification',\n",
        "                      'priority', 'protocol','src_ip', 'src_port', 'dst_ip', 'dst_port']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for match in pattern.finditer(log_data):\n",
        "          writer.writerow(match.groupdict())\n",
        "\n",
        "      return f\"The file has been converted to csv successfully.\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "  def convert_json_csv(self, json_file, csv_file):\n",
        "    try:\n",
        "      print(\"Start converting JSON to CSV...\")\n",
        "      with open(json_file, 'r') as f:\n",
        "        data = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "      fieldnames = set()\n",
        "      for row in data:\n",
        "          fieldnames.update(row.keys())\n",
        "\n",
        "      fieldnames = list(fieldnames)\n",
        "\n",
        "      with open(csv_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(fieldnames)\n",
        "\n",
        "        for row in data:\n",
        "          writer.writerow(row.get(field, '') for field in fieldnames)\n",
        "\n",
        "      return f\"The file has been converted to csv successfully.\"\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"An error occurred: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "a = PcapFileProcessing()\n",
        "#a.convert_log_csv( log_file=\"/content/drive/MyDrive/hybrid_IDS/fast.log\",\n",
        "#                  csv_file=\"/content/drive/MyDrive/hybrid_IDS/alerts.csv\")\n",
        "\n",
        "#a.convert_json_csv(json_file='/content/drive/MyDrive/hybrid_IDS/eve.json',\n",
        "#                   csv_file='/content/drive/MyDrive/hybrid_IDS/eve.csv')\n",
        "\n",
        "#a.delete_data( file_path='/content/drive/MyDrive/hybrid_IDS/eve.csv')\n",
        "a.date_time_columns(file_path='/content/drive/MyDrive/hybrid_IDS/eve.csv',\n",
        "                    csv_file='/content/drive/MyDrive/hybrid_IDS/dataset/eve.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s_LHnGmW20vr",
        "outputId": "9b5a8e67-06a8-4cbb-af83-2173bbed7543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'تم تحويل timestamp بنجاح إلى date و time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}