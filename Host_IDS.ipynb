{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hananbahtiti/Hybrid-Intrusion-detection-Systems/blob/main/Host_IDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== IMPORT LIBRARIES ==========\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Scikit-learn for preprocessing and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "# TensorFlow and Keras for deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Suppress TensorFlow warnings for cleaner output\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# ========== SETTINGS ==========\n",
        "# Base directory for saving models and outputs\n",
        "BASIC_FOLDER = '/content/drive/MyDrive/hybrid_IDS/host'\n",
        "\n",
        "# Dataset path (must include 'Label' column)\n",
        "DATA_PATH = f'/content/drive/MyDrive/hybrid_IDS/dataset/all/balanced_attack.csv'\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_CSV = f'{BASIC_FOLDER}/model/anomaly_detection_results.csv'\n",
        "MODEL_PATH = f'{BASIC_FOLDER}/model/autoencoder_model.keras'\n",
        "PLOT_DIR = f'{BASIC_FOLDER}/model/'\n",
        "os.makedirs(PLOT_DIR, exist_ok=True)  # Create output directory if it doesn't exist\n",
        "\n",
        "# ========== LOAD & PREPARE DATA ==========\n",
        "# Load dataset\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Split into features (X) and labels (y)\n",
        "X = data.drop(columns=['Label'])\n",
        "y = data['Label']\n",
        "\n",
        "# Normalize features for better training performance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Use only 'normal' samples (label == 0) for training the autoencoder\n",
        "X_normal = X_scaled[y == 0]\n",
        "\n",
        "# Split normal data into training and validation sets\n",
        "X_train, X_val = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== AUTOENCODER MODEL ==========\n",
        "# Define input shape\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Build the autoencoder architecture\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "encoded = Dense(16, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='linear')(decoded)  # Linear activation to reconstruct input\n",
        "\n",
        "# Compile the autoencoder model\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')  # Mean Squared Error for reconstruction\n",
        "\n",
        "# Define callbacks: early stopping and model checkpointing\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor='val_loss', verbose=1)\n",
        "\n",
        "# ========== TRAINING ==========\n",
        "# Train the autoencoder using only normal samples\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, X_val),\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, model_checkpoint]\n",
        ")\n",
        "print(f\"[✓] Model saved to: {MODEL_PATH}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU64lvKJ3OFf",
        "outputId": "5151e04e-9da4-4c85-eb96-02874be3954c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m144185/225000\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 2ms/step - loss: 0.0032"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "deMgvJx0zdqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== PREDICTION ==========\n",
        "# Reconstruct all samples (both normal and anomalous)\n",
        "X_pred = autoencoder.predict(X_scaled)\n",
        "\n",
        "# Calculate reconstruction error (Mean Squared Error per sample)\n",
        "mse = np.mean(np.power(X_pred - X_scaled, 2), axis=1)\n",
        "\n",
        "# Define anomaly threshold based on 95th percentile of normal reconstruction errors\n",
        "threshold = np.percentile(mse[y == 0], 95)\n",
        "\n",
        "# Classify as anomaly if error exceeds threshold\n",
        "predictions = (mse > threshold).astype(int)\n",
        "\n",
        "# ========== SAVE PREDICTIONS ==========\n",
        "# Append reconstruction error and predictions to the original dataset\n",
        "output_df = data.copy()\n",
        "output_df['reconstruction_error'] = mse\n",
        "output_df['predicted'] = predictions\n",
        "\n",
        "# Save results to CSV\n",
        "output_df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"[✓] Predictions saved to: {OUTPUT_CSV}\")"
      ],
      "metadata": {
        "id": "E5erKQK04ku-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== METRICS ==========\n",
        "# Ground truth labels\n",
        "y_true = y.values\n",
        "\n",
        "# Evaluate model using AUC and F1 Score\n",
        "auc = roc_auc_score(y_true, mse)  # Continuous score-based metric\n",
        "f1 = f1_score(y_true, predictions)  # Binary classification metric\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"AUC Score: {auc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, predictions))\n",
        "\n",
        "# ========== CONFUSION MATRIX HEATMAP ==========\n",
        "# Generate and plot confusion matrix\n",
        "conf_mat = confusion_matrix(y_true, predictions)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Save confusion matrix plot\n",
        "conf_path = os.path.join(PLOT_DIR, 'confusion_matrix.png')\n",
        "plt.savefig(conf_path)\n",
        "print(f\"[✓] Confusion matrix plot saved to: {conf_path}\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# ========== PLOT TRAINING LOSS ==========\n",
        "# Plot training and validation loss over epochs\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Autoencoder Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save loss plot\n",
        "loss_path = os.path.join(PLOT_DIR, 'training_loss.png')\n",
        "plt.savefig(loss_path)\n",
        "print(f\"[✓] Loss plot saved to: {loss_path}\")\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# ========== PLOT RECONSTRUCTION ERROR DISTRIBUTION ==========\n",
        "# Visualize how reconstruction errors are distributed for normal vs anomalous samples\n",
        "error_df = pd.DataFrame({'reconstruction_error': mse, 'true_label': y_true})\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.histplot(error_df[error_df['true_label'] == 0]['reconstruction_error'],\n",
        "             bins=50, color='blue', label='Normal', stat='density')\n",
        "sns.histplot(error_df[error_df['true_label'] == 1]['reconstruction_error'],\n",
        "             bins=50, color='red', label='Attack', stat='density')\n",
        "plt.axvline(threshold, color='black', linestyle='--', label='Threshold')\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save error distribution plot\n",
        "error_path = os.path.join(PLOT_DIR, 'reconstruction_error_distribution.png')\n",
        "plt.savefig(error_path)\n",
        "print(f\"[✓] Error distribution plot saved to: {error_path}\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "bCTJ_zhl3SeA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}